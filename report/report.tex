\documentclass[11pt,a4paper,oneside]{article}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage[utf8]{inputenc}
\usepackage[export]{adjustbox}

%if you don't like the style, comment from here
\setlength{\textwidth}{16cm}
\setlength{\marginparwidth}{1.5cm}
\setlength{\parindent}{0cm}

\setlength{\parskip}{0.15cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{\oddsidemargin}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{0cm}
\setlength{\headsep}{0cm}

\renewcommand{\familydefault}{\sfdefault}
%until here to see the old style

\begin{document}
\graphicspath{ {images/} }
\title{Designing an Index for ZooDB}
\author{Jonas Nick \& Bogdan Vancea}
%\date{December 2004}
\maketitle
\tableofcontents
\clearpage

\section{Introduction}
ZooDB is an open source object database written in Java. It is based on the JDO standard, which is a specification of Java object persistence.
Up to this point it lacks some advanced features like concurrency and schema evolution but it has already been applied successfully in several university projects. 
The most important selling point of ZooDB is its speed. It is up to four times faster than competitor db4o on the PolePosition benchmark\footnote{http://polepos.org/}.

\subsection{Database Index}
A database \emph{index} is a data structure for rapidly retrieving and saving key-value entries. 
It allows ordered iteration and can be stored in a hard drive file.

The most natural choice for the underlying datastructure of our index are \emph{B+ trees}.
It consists of inner nodes which store keys and pointer to children nodes and leaf nodes consisting of keys and values.
Each node fills exactly one disk page enabling efficient storage in a file system.
Therefore, there is a maximum number of entries that a node can hold -- this number is often called \emph{order} of the tree.
On the other hand, there is a minimum number of entries that are allowed in a node in order to keep the tree balanced and allocate as few disk pages as possible.
This lower bound is normally half the order.

When inserting or deleting an entry, a node might become \emph{over- or underfull} and the tree has to be recursively rebalanced again.
If the node is overfull, the node is split into two nodes and the middle key is put into the parent node.
After deleting an entry and the node becoming underfull, the node is merged with a sibling node. If a merge is not possible, some entries are redistributed from a sibling to the underflowing node.
It is important to note that insert, delete and rebalancing operations have logarithmic time complexity.

In ZooDB a user can create an index on an attribute of the object he wants to store to permit fast point and range queries for that attribute.
Additionally, ZooDB uses indexes internally at several other places.
It uses for example the ObjectID index to map OIDs to positions on the disk and the Extension Index to map a disk position to the following disk position if an object spans several disk pages. 

\section{Goals \& Challenges}
ZooDB already had a working index implementation, the goal was that the reimplementation should be significantly faster. 
Since indexes operate in multiple roles in the database, the speed of the implementation has a substantial impact on the performance of the whole database
The runtime of the index is largely influenced by disk access, therefore the implementation should minimize the number of nodes nodes.
Also, nodes should be rarely modified, because in that case they have to be written to disk.

Additionally, the index should support caching via a \emph{buffer manager}. 
Basically, the buffer manager is responsible for fetching pages from disk and providing them to the tree.
This allows not loading the whole tree into the memory, but restricting the cache to a fixed size and load and unload pages as needed.

There should be two types of indexes, one is called \emph{key unique} and the other \emph{key-value unique} (or non-unique). 
Consider for example inserting first the key-value entry $(1,1)$ and subsequently inserting $(1,2)$.
The difference is that in a key unique index the first entry will be overwritten, but in a key-value unique index both entries remain stored. 

Unfortunately, the new features are costly: prefix-sharing encoding takes some additional time and caching adds another layer of indirection.
In order to get the best performance out of the code, we dealt with low level implementation optimizations like replacing multiplications and divisions with bit-shifts and reducing polymorphism.

Textbooks do a good job of explaining the big picture of the B+ tree operations but miss some implementation details.
For example, in their description leafs are linked through sibling pointers. Since ZooDB is a copy-on-write database, sibling pointers are not possible.
Pages are seldomly stored on the same place twice, so if you store leaf $l_1$, you have to update the sibling pointer of $l_2$, store $l_2$, update the sibling pointer of $l_1$ and so on.
%Furthermore, textbooks do not cover duplicate entries at all, The operations for key-value unique trees are in many steps different.
%different order
%and they do not cover prefix sharing 

\subsection{Prefix sharing}
\begin{figure}[ht]
\center
\begin{tikzpicture}
\node[draw,align=left] at (0,0) {00010000\\ 00010100\\ 00010110};
\draw (0.2,-1) -- (0.2,1);
\draw [->] (1,0) -- (2,0);
\end{tikzpicture}
\begin{tikzpicture}
\node[draw,align=right] at (0,0) {00010\\ 000\\100\\110};
\end{tikzpicture}
\caption{The concept behind prefix sharing}
\label{fig:prefix-sharing}
\end{figure}

From the start on, the most important idea to speed up the reimplementation was to make use of \emph{prefix sharing}.
Imagine the left box in figure \ref{fig:prefix-sharing} represents a node which has some keys shown as bit strings.
All keys in the node have a prefix in common, so the same information is contained if we store the prefix and the additional bits (box on the right).
It is easy to see that less bits have to be stored now and therefore more keys can be packed on a page.
However, prefix sharing entails a lot of changes to the operations of the BTree. 
Because the prefix, and specifically its length, depends on the actual keys in a node, the maximum number of entries is different for every node.
This means that prefix sharing also affects all rebalancing operations.


\section{Implementation}
\subsection{Buffer Manager} % (fold)
\label{sub:buffer_manager}
Explain how all accesses to nodes are done through the BufferManager.

\subsection{Class Diagram} % (fold)
\label{sub:class_diagram}

For the new implementation of the index we have focused on achieving a balance between good design and code that offers good performance. Figure \ref{fig:class-diagram} show the class diagram for ZooDB.  
\begin{figure}[ht]
\includegraphics[scale=0.089, center]{ZooDBClassDiagram} 
\caption{The class diagram for the new index implementation}
\label{fig:class-diagram}
\end{figure}

% subsection subsection_name (end)
The \textbf{BTree} class implements all the logic for the B+ tree operations, including the re-balancing operations. The operations implemented operate on instances of the \textbf{BTreeNode} class. The \textbf{BTreeNode} abstract class contains the logic for a B+ tree node. To reduce the usage of polymorphism, we have opted not to implement different classes for leaf, inner or root nodes. The \textbf{BTreeNode} class has additional boolean fields, \textit{isLeaf} and \textit{isRoot} that are used to describe the position of a node within the tree. This choice reduces the size of the inheritance hierarchy, however in the cases where leaf nodes have to be treated differently than inner nodes we have added additional if statements to separate the behavior. 

The \textbf{BTreeNode} class holds no information about the child nodes of the node. However, this class exposes a number of abstract child access methods, like \textit{getChild()} or \textit{getChildren()}. These methods are implemented by the \textbf{PagedBTreeNode} class. Additionally, the \textbf{BTreeNode} class does not hold any information to differentiate between nodes belonging to unique and non-unique trees. All of the methods of for the \textbf{BTree} and \textbf{BTreeNode} classes which need to have different behaviour for unique and non-unique trees are abstract methods and will be overridden by the \textbf{UniquePagedBTreeNode} and \textbf{NonUniquePagedBTreeNode} classes.

The \textbf{PagedBTreeNode} holds the logic responsible for loading nodes from storage. class extends the \textbf{BTreeNode} and contains a reference to the BufferManager associated with the tree. Each instance of \textbf{PagedBTreeNode} contains a \textit{pageId}, which is given assigned by the \textbf{BufferManager} upon storage. The \textbf{PagedBTree} node contains information about the children of a node. We have opted to store the an array consisting of the page ids of the child nodes, instead of an array of hard references to the PagedBTreeNodes. Instead we store an array of weak references to the \textbf{PagedBTreeNode} objects. When a new node is loaded from disk, the child array is populated with the page ids of its child nodes while the array of weak references to the children is initialized to an array of null references. The first access to a child node will retrieve the \textbf{PagedBTreeNode} instance from the buffer manager and populate the weak references corresponding to its page id. Subsequent accesses to the child will use the weak reference, provided the child objects is not reclaimed by the garbage collector. If any of the children nodes are reclaimed by the garbage collector, they will be retrieved from storage on the next access and the weak reference will be repopulated.

The \textbf{UniquePagedBTreeNode} and \textbf{NonUniquePagedBTreeNode} classes extend the \textbf{PagedBTreeNode} class and contain data and behaviour specific to nodes belonging to unique and non-unique trees. For example, in case the non-unique nodes, inner nodes cotains the both a key array and a value array. This leads to different behaviour for unique and non-unique nodes during operations like peforming binary searches, shifting node entries to the left or right or copying node entries from one node to the other. This different behaviour is implemented in the aforementioned classes: \textbf{UniquePagedBTreeNode} and \textbf{NonUniquePagedBTreeNode}. 

The tree iterators are implemented by the \textbf{AscendingBTreeLeafEntryIterator} and \textbf{DescendingBTreeLeafEntryIterator} classes. The shared behaviour for these classes is implemented in the abstract class \textbf{BTreeLeafEntryIterator}. Because we have opted not to store information about the parent nodes, neither in the form of page ids or references, the iteration process requires the use of a stack to store the current position in the tree. The iterators do not support \textbf{Copy on Write} and are invalidated when the B+ tree is modified. 

The indexes are implemented in the \textbf{UniqueBTreeIndex} and \textbf{NonUniqueBTreeIndex} classes. As the index interface offers both methods to modify the B+ tree and to create iterators, we have opted to separate the tree logic in \textbf{BTree} and the iterator in \textbf{BTreeLeafEntryIterator}. This means that index objects delete all B+ tree operations to a reference of a \textbf{BTree} and create instances of \textbf{BTreeLeafEntryIterator} as needed.

The prefix sharing logic has been implemented in the \textbf{PrefixSharingHelper} class. Operations like computing the prefix, encoding/decoding the key array using prefix sharing and the re-balancing computations are available as static methods. We have choses to keep all this logic static to avoid creating extra computational objects that would have to be passed to \textbf{BTreeNode} objects upon creation.
% subsection class_diagram (end)

\subsection{B+ Tree Operations} % (fold)
\label{sub:b_tree_operations}
One important implementation choice we have made is to store the key array un-encoded in memory and to perform the prefix sharing encoding only when nodes are written to disk. While this approach increase the size of the node pages when loaded in memory, the advantage of this choice is that the costly key array encoding is only performed once, even if the node is modified multiple times before it is written to disk. 

Another advantage of having the key array unencoded in memory is that the search operations is similar to the search operation run on a traditional B+ tree. 

For the insert operation, if the new key has been inserted in the first or the last position of the key array, the prefix for the node has to be recomputed. The overflow is handled as follows:
\begin{enumerate}
	\item The algorithm first attempts to redistribute some keys from the current node to its left sibling. We do this to delay the moment when a new node is created. The exact redistribution process is described in the next section.
	\item If that is not possible, the current node is split into 2 nodes of equal size. One important different appears in how the new tree handles keys moving from one node to the other. In a traditional B+ tree, the max number of keys on a node is fixed, which makes it very to perform operations like splitting a node into 2 nodes of equal sizes. In case of prefix sharing, the first half of the keys could have a very different prefix size compared to the second half, which would lead to having 2 nodes with very different sizes. To avoid this, we perform a binary search to compute the optimal number of keys to move to the new node.
\end{enumerate}
	
In the case of the delete operation, the removal of an entry from the key array can cause the prefix to change. If the node from which the deletion has been performed is now underfull, a re-balancing procedure begin. The steps for the re-balancing are the following:
\begin{enumerate}
	\item In the first step, the algorithm first checks if the current node can be merged with \textbf{either} its left or its right neighbour. 
	\item If the merge at the previous step is not possible, the next step is to check wether the keys from the current node can be split between its left sibling \textbf{and} its right sibling. While this approach would involve in marking both the left and righ siblings as dirty, the current node can then be removed.
	\item If the the previous operation was not possible, some keys are redistribute from either the left or the right sibling. As a note, the success of the rebalancing operations is not guaranteed and the current node could remain underfull after the re-balance operation. However, we observed that this behaviour does not happen very often and does not have a significant impact on the tree size.
\end{enumerate}

% subsection b_tree_operations (end)

\subsection{Prefix Sharing Implementation} % (fold)
\label{sub:prefix_sharing_rebalancing}
When storing a node, the key array of a node is encoded into an array of bytes accoding to the common prefix. The encoding was peformed at the bit-level, meaning that both the binary prefix and the deviations of elements from the prefix are computed as arbitrary number of bits. This means that if the binary prefix has length 9, it can be store in 2 bytes. Moreover, the remaining 7 bits from the second byte can be used to store part of the deviation of the fist array element. Other available alternatives for encoding would have been to compute both the prefix and the deviations as multiples of 8 and ensure that each encoded byte contains only prefix bits or deviation bits. We have chosen the bit-level encoding because it offers the best space reduction.

We have based the prefix sharing algorithms on the following observations about the binary prefix for an array of \textbf{sorted} integer elements.
\begin{itemize}
	\item The binary prefix of the array is equal to the binary prefix shared by the first and last element.
	\item Inserting a new element in the array in an ordered manner will either decrease the prefix of the whole array or leave it unchanged.
	\item Removing an element from the array will either increase the prefix of the whole array or leave it unchanged.
\end{itemize}

The main purpose of the split operation is to split the keys of a node that is overflowing between 2 nodes such that the node sizes are roughly the same. We perform this operation using a binary search to determine the optimal number of keys that have be moved from the current node such that the sizes of the current node and new node are as close as possible. The initial value for the number of keys to move is equal to half of the number of keys stored on the current node. The following logic is executed during each round of the binary search. 
\begin{enumerate}
	\item Compute the binary prefixes and sizes for the nodes that would result if the current number of keys k would be moved to a 
	new node. 
	\item Compute the different between the node sizes and if the current split is the best encountered so far, store the current number of keys to move and the size difference.
	\item If the size of the current node after the simulated key move is smaller than the size of the new node, decrease the number of keys to move. Otherwise, increase the number of keys to move.
\end{enumerate}
In the end, the split algorithm returns the number of keys to move corresponding to the mimimum different in size between the current and new node.

A special case of the split operation is when the current node overflow and we would like to check if the key array can be equally split between the left and right siblings. In that case, the binary search is modified to search for number of keys to move to the right node, the remaining keys being moved to the left node. An additional optimization that we have added in that case is to stop at the first split that results in both left and right nodes having a valid size.

The goal of the redistribute operation is to move a number of keys from a node that is not underfull to an underfull node. It is desirable that at the end the split operation, neither of the nodes be underfull. If that is not possible, the redistribution fails and no keys are moved. The number of keys to move to the underfull node is computed in a similar manner to the split operation, using a binary search. Therefore, at each step, the prefixes and the sizes of the node that would result after the operation are computed. The binary search stops at the first split that results into two nodes of valid sizes.

The goal of the merge operation is reduce the number of nodes in the tree by merging an underfull node into either its left or right neighbours. In the case of the prefix sharing, the check that determines if two nodes can be merged is non-trivial, as it involves computing the prefix and the size of the node that would result after the merge. If the new node does not fit into a page file, the merge operation will not be performed.

\subsection{Support for different sizes}
We have support for this (ex: PagedPosIndex) , maybe we can provide some additional details.


% subsection prefix_sharing_rebalancing (end)


\end{document}
